name: LLM-ExtractPrompt-v1
type: feature
description: |
  This feature tries to extract the user prompt the last entry in the `contents.parts`
  request collection. If an input prompt is found, then the number of tokens is estimated
  using a local javascript method.

  ## Prerequisites
  None.

  ## Inputs
  - User prompt request content in this format:
    ```json
    {
      "contents": [
        {
          "role": "user",
          "parts": [
            {
              "text": "why is the sky blue?"
            }
          ]
        }
      ]
    }
    ```

  ## Outputs
  - If a prompt is found, it is written to the Apigee variable **llm.inputPrompt**.
  - If a prompt is found, the estimated token length is written to the Apigee variable **llm.promptEstimatedTokenCount**.
parameters: []
endpointFlows:
  - name: PreFlow
    mode: Request
    steps:
      - name: JS-EstimateTokens
targetFlows: []
endpoints: []
targets: []
policies:
  - name: JS-EstimateTokens
    type: Javascript
    content:
      Javascript:
        _attributes:
          continueOnError: "false"
          enabled: "true"
          timeLimit: "200"
          name: JS-EstimateTokens
        DisplayName:
          _text: JS-EstimateTokens
        Properties: {}
        ResourceURL:
          _text: jsc://estimate-tokens.js
resources:
  - name: estimate-tokens.js
    type: jsc
    content: |-
      var promptInput = "";
      var requestObject = request.content.asJSON;
      if (requestObject["contents"]) {
        // gemini format
        const userConversations = requestObject.contents.filter(item => item.role === 'user');
        const lastUserConversation = userConversations[userConversations.length - 1];
        promptInput = lastUserConversation.parts[lastUserConversation.parts.length - 1].text;
      } else if (requestObject["messages"]) {
        // openapi format
        const userConversations = requestObject.messages.filter(item => item.role === 'user');
        promptInput = userConversations[userConversations.length - 1].content;
      }

      context.setVariable("llm.promptInput", promptInput);
      if (promptInput) {
        var word_count = promptInput.split(" ").length;
        var char_count = promptInput.length;

        var tokens_count_word_est = word_count / 0.75;
        var tokens_count_char_est = char_count / 4.0;

        var result = 0;

        if (tokens_count_word_est > tokens_count_char_est)
          result = tokens_count_word_est;
        else if (tokens_count_char_est > tokens_count_word_est)
          result = tokens_count_word_est;

        context.setVariable("llm.promptEstimatedTokenCount", result);
        context.setVariable("quota.weight", result);
      }
