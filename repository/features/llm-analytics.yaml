name: llm-analytics
type: feature
description: |
  Adds LLM data to data collectors. This feature does not fail if properites are not found,
  however the data collectors must be created in your Apigee org before using it.

  **Prerequisites**
  - Apigee data collectors **llm_model**, **llm_prompt_token_count**, **llm_response_token_count** and **llm_total_token_count**
  created in your org before using this feature. You can create these by running these commands with [apigeecli]().
  ```sh
  PROJECT_ID=YOUR_APIGEE_PROJECT
  apigeecli datacollectors create -d "Model name" -n dc_llm_model -p STRING --org "$PROJECT_ID" --token $(gcloud auth print-access-token)
  apigeecli datacollectors create -d "Total token count" -n dc_llm_total_token_count -p INTEGER --org "$PROJECT_ID" --token $(gcloud auth print-access-token)
  apigeecli datacollectors create -d "Prompt token count" -n dc_llm_prompt_token_count -p INTEGER --org "$PROJECT_ID" --token $(gcloud auth print-access-token)
  apigeecli datacollectors create -d "Total token count" -n dc_llm_response_token_count -p INTEGER --org "$PROJECT_ID" --token $(gcloud auth print-access-token)
  ```

  **Inputs**
  - Apigee variables **llm.model**, **llm.promptTokenCount**, **llm.responseTokenCount**, **llm.totalTokenCount**,
  typically filled from an LLM feature.

  **Outputs**
  - Data collectors are written with Apigee variable data, if found.
categories:
  - llm
parameters: []
defaultEndpoint:
  name: default
  basePath: /feature-llm-datacollector-v1
  routes:
    - name: default
  flows:
    - name: PostFlow
      mode: Response
      steps:
        - name: DC-ModelInfo
  faultRules: []
endpoints: []
targets: []
policies:
  - name: DC-ModelInfo
    type: DataCapture
    content:
      DataCapture:
        _attributes:
          name: DC-ModelInfo
          continueOnError: "false"
          enabled: "true"
        DisplayName:
          _text: DC-ModelInfo
        IgnoreUnresolvedVariables:
          _text: "true"
        Capture:
          - Collect:
              _attributes:
                ref: llm.model
                default: ""
            DataCollector:
              _text: dc_llm_model
          - Collect:
              _attributes:
                ref: llm.promptTokenCount
                default: "0"
            DataCollector:
              _text: dc_llm_prompt_token_count
          - Collect:
              _attributes:
                ref: llm.responseTokenCount
                default: "0"
            DataCollector:
              _text: dc_llm_response_token_count
          - Collect:
              _attributes:
                ref: llm.totalTokenCount
                default: "0"
            DataCollector:
              _text: dc_llm_total_token_count
resources: []
tests: []
