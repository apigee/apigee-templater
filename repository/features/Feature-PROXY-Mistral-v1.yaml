name: Feature-PROXY-Mistral-v1
type: feature
description: |
  This feature adds an endpoints with basepath at v1/mistral, and proxies
  requests to the Vertex AI Mistral endpoint in a configured region.

  ## Prerequisites
  - Vertex AI is enabled in the GCP Project.
    ```sh
    gcloud services enable aiplatform.googleapis.com
    ```
  - A Mistral model is enabled in the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/publishers/mistralai/model-garden/mistral-medium-3).

  ## Inputs
  - Request prompt input in the OAI format:
    ```js
    {
      "messages": [{
        "role": "user",
        "content": "Why is the sky blue?"
      }],
      "stream": true
    }
    ```

  ## Outputs
  - Stores successful prompt token count in the **Apigee variable llm.promptTokenCount**
  - Stores successful response token count in the **Apigee variable llm.responseTokenCount**
  - Stores successful total token count in the **Apigee variable llm.totalTokenCount**
  - Normal response from Mistral model in OAI format
parameters:
  - name: PROJECT_ID
    displayName: PROJECT_ID
    description: The GCP project id that should be used for the Vertex AI Gemini endpoint.
    default: YOUR_PROJECT_ID
    examples:
      - YOUR_PROJECT_ID
  - name: REGION
    displayName: REGION
    description: The GCP region that should be used for the Vertex AI endpoint.
    default: europe-west4
    examples:
      - europe-west4
      - us-central1
  - name: MODEL_ID
    description: Configuration input for MODEL_ID
    default: mistral-medium-3
    examples:
      - mistral-medium-3
      - mistral-small-2503
  - name: MISTRAL_PATH
    displayName: MISTRAL_PATH
    description: Configuration input for MISTRAL_PATH
    default: europe-west4-aiplatform.googleapis.com/v1/projects/apigee-prod13/locations/europe-west4/publishers/mistralai/models/mistral-medium-3:rawPredict
    examples: []
endpoints:
  - name: mistral
    basePath: /v1/mistral
    routes:
      - name: default
        target: mistral
    flows: []
    faultRules: []
targets:
  - name: mistral
    url: https://{propertyset.mistral.MISTRAL_PATH}
    auth: GoogleAccessToken
    scopes:
      - https://www.googleapis.com/auth/cloud-platform
    flows:
      - name: PreFlow
        mode: Request
        steps:
          - name: JS-SetMistralModel
          - name: JS-SetMistralPrompt
            condition: llm.promptInput != null
      - name: PostFlow
        mode: Response
        steps:
          - name: EV-MistralStoreInfo
      - name: EventFlow
        mode: Response
        steps:
          - name: JS-SetMistralStreamInfo
    faultRules: []
    httpTargetConnection:
      Properties: {}
      URL:
        _text: https://{propertyset.mistral.MISTRAL_PATH}
      Authentication:
        GoogleAccessToken:
          Scopes:
            Scope:
              _text: https://www.googleapis.com/auth/cloud-platform
policies:
  - name: EV-MistralStoreInfo
    type: ExtractVariables
    content:
      ExtractVariables:
        _attributes:
          continueOnError: "false"
          enabled: "true"
          name: EV-MistralStoreInfo
        DisplayName:
          _text: EV-MistralStoreInfo
        Properties: {}
        IgnoreUnresolvedVariables:
          _text: "true"
        JSONPayload:
          Variable:
            - _attributes:
                name: model
              JSONPath:
                _text: $.model
            - _attributes:
                name: promptTokenCount
              JSONPath:
                _text: $.usage.prompt_tokens
            - _attributes:
                name: responseTokenCount
              JSONPath:
                _text: $.usage.completion_tokens
            - _attributes:
                name: totalTokenCount
              JSONPath:
                _text: $.usage.total_tokens
        Source:
          _attributes:
            clearPayload: "false"
          _text: response
        VariablePrefix:
          _text: llm
  - name: JS-SetMistralModel
    type: Javascript
    content:
      Javascript:
        _attributes:
          continueOnError: "false"
          enabled: "true"
          timeLimit: "200"
          name: JS-SetMistralModel
        DisplayName:
          _text: JS-SetMistralModel
        Properties: {}
        ResourceURL:
          _text: jsc://set-mistral-model.js
  - name: JS-SetMistralPrompt
    type: Javascript
    content:
      Javascript:
        _attributes:
          continueOnError: "false"
          enabled: "true"
          timeLimit: "200"
          name: JS-SetMistralPrompt
        DisplayName:
          _text: JS-SetMistralPrompt
        Properties: {}
        ResourceURL:
          _text: jsc://set-mistral-prompt.js
  - name: JS-SetMistralStreamInfo
    type: Javascript
    content:
      Javascript:
        _attributes:
          continueOnError: "false"
          enabled: "true"
          timeLimit: "200"
          name: JS-SetMistralStreamInfo
        DisplayName:
          _text: JS-SetMistralStreamInfo
        Properties: {}
        ResourceURL:
          _text: jsc://set-mistral-stream-info.js
resources:
  - name: set-mistral-model.js
    type: jsc
    content: |-
      var requestObject = request.content.asJSON;
      var model = context.getVariable("propertyset.mistral.MODEL_ID");

      if (requestObject) {
        requestObject["model"] = model;
      }

      context.setVariable('request.content', JSON.stringify(requestObject));
  - name: set-mistral-prompt.js
    type: jsc
    content: |-
      var requestObject = request.content.asJSON;
      var inputPrompt = context.getVariable("llm.promptInput");

      if (requestObject && inputPrompt) {
        requestObject = setLastPrompt(requestObject, inputPrompt);
        context.setVariable("request.content", JSON.stringify(requestObject));
      }

      function setLastPrompt(requestObject, inputPrompt) {
        if (requestObject && requestObject["contents"]) {
          // gemini format
          var userConversations = requestObject.contents.filter(item => item.role === 'user');
          if (userConversations.length > 0) {
            var lastUserConversation = userConversations[userConversations.length - 1];
            if (lastUserConversation && lastUserConversation.parts && lastUserConversation.parts.length > 0)
              lastUserConversation.parts[lastUserConversation.parts.length - 1].text = inputPrompt;
          }
        } else if (requestObject && requestObject["messages"]) {
          // openapi format
          var userConversations = requestObject.messages.filter(item => item.role === 'user');
          if (userConversations && userConversations.length > 0) {
            var lastUserConversation = userConversations[userConversations.length - 1];
            if (lastUserConversation.content && Array.isArray(lastUserConversation.content) && lastUserConversation.content.length > 0) {
              // array of content
              const contentTexts = lastUserConversation.content.filter(item => item.type === 'text');
              if (contentTexts && contentTexts.length > 0) {
                contentTexts[contentTexts.length - 1].text = inputPrompt;
              }
            } else if (lastUserConversation.content) {
              lastUserConversation.content = inputPrompt;
            }
          }
        }

        return requestObject;
      }
  - name: set-mistral-stream-info.js
    type: jsc
    content: |-
      var streamString = context.getVariable("response.event.current.content");
      if (streamString) {
        streamString = streamString.replace("data: ", "");
        if (streamString != "[DONE]") {
          var streamData = JSON.parse(streamString);

          if (streamData["model"])
            context.setVariable("llm.model", streamData["model"]);

          if (streamData["usage"] && streamData["usage"]["prompt_tokens"])
            context.setVariable("llm.promptTokenCount", streamData["usage"]["prompt_tokens"]);
          
          if (streamData["usage"] && streamData["usage"]["completion_tokens"])
            context.setVariable("llm.responseTokenCount", streamData["usage"]["completion_tokens"]);

          if (streamData["usage"] && streamData["usage"]["total_tokens"])
            context.setVariable("llm.totalTokenCount", streamData["usage"]["total_tokens"]);
        }
      }
  - name: mistral.properties
    type: properties
    content: |
      MODEL_ID={MODEL_ID}
      MISTRAL_PATH={MISTRAL_PATH}
tests: []
